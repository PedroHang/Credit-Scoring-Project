{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TITLE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from scipy.stats import ks_2samp\n",
    "from scipy.stats import t\n",
    "import matplotlib.ticker as mticker\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, auc\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import precision_recall_curve, accuracy_score\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import make_scorer, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "plt.style.use('ggplot')default, 1: Defaulted on a loan or payment</td></tr>\r\n",
    "</table>\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 bucket and object key\n",
    "bucket_name = 'dataset-content-pedrohang'\n",
    "object_key = 'credit-scoring/credit_scoring.ftr'\n",
    "\n",
    "# Constructing the S3 URI\n",
    "s3_uri = f's3://{bucket_name}/{object_key}'\n",
    "\n",
    "# Reading the feather file from S3\n",
    "df = pd.read_feather(s3_uri, storage_options={'anon': True})\n",
    "\n",
    "df.drop(columns=['index'], inplace=True)\n",
    "\n",
    "df['default'] = df.default.astype('int64')\n",
    "\n",
    "df['not_default'] = 1 - df['default']\n",
    "\n",
    "def drop_missing_values(my_df):\n",
    "    # Drop rows with any missing values\n",
    "    my_df.dropna(inplace=True)\n",
    "    \n",
    "    return my_df\n",
    "\n",
    "df_filled = drop_missing_values(df)\n",
    "\n",
    "df_train = df_filled[df_filled['reference_date'] <= '2015-12-31']\n",
    "df_oot = df_filled[df_filled['reference_date'] >= '2016-01-01']misleading the model's performance and interpretation.on\n",
    "\n",
    "def IV(x, y):\n",
    "    # Create a crosstab to calculate the frequency distribution of x and y\n",
    "    table = pd.crosstab(x, y, margins=True, margins_name='total')\n",
    "\n",
    "    # Get the labels for the event and non-event categories\n",
    "    event_label = table.columns[0]\n",
    "    non_event_label = table.columns[1]\n",
    "\n",
    "    # Calculate the percentage of events\n",
    "    table['pct_event'] = table[event_label] / table.loc['total', event_label]\n",
    "    table['event_proportion'] = table[event_label] / table.loc['total', event_label]\n",
    "\n",
    "    # Calculate the percentage of non-events\n",
    "    table['pct_non_event'] = table[non_event_label] / table.loc['total', non_event_label]\n",
    "\n",
    "    # Calculate Weight of Evidence (WoE)\n",
    "    table['woe'] = np.log(table.pct_event / table.pct_non_event)\n",
    "\n",
    "    # Calculate the partial Information Value (IV)\n",
    "    table['iv_partial'] = (table.pct_event - table.pct_non_event) * table.woe\n",
    "\n",
    "    # Return the sum of the partial IVs as the total Information Value\n",
    "    return table['iv_partial'].sum()\n",
    "\n",
    "metadata = pd.DataFrame(df_filled.dtypes, columns=['dtype'])\n",
    "metadata['nmissing'] = df_filled.isna().sum()\n",
    "metadata['unique_values'] = df_filled.nunique()\n",
    "metadata['role'] = 'covariate'\n",
    "metadata.loc['default','role'] = 'response'\n",
    "metadata.loc['not_default','role'] = 'response'\n",
    "metadata\n",
    "\n",
    "for var in metadata[metadata.role=='covariate'].index:\n",
    "    if  (metadata.loc[var, 'unique_values']>6):\n",
    "        metadata.loc[var, 'IV'] = IV(pd.qcut(df_filled[var],5,duplicates='drop'), df_filled.default)\n",
    "    else: \n",
    "        metadata.loc[var, 'IV'] = IV(df_filled[var], df_filled.default)\n",
    "\n",
    "    \n",
    "metadata\n",
    "\n",
    "def biv_discrete(var, df_filled):\n",
    "    df_filled['not_default'] = 1 - df_filled.default\n",
    "    g = df_filled.groupby(var)\n",
    "\n",
    "    biv = pd.DataFrame({'qt_not_default': g['not_default'].sum(),\n",
    "                        'qt_default': g['default'].sum(),\n",
    "                        'default': g['default'].mean(),\n",
    "                        var: g['default'].mean().index, \n",
    "                        'cont': g[var].count()})\n",
    "    \n",
    "    biv['ep'] = (biv.default * (1 - biv.default) / biv.cont) ** .5\n",
    "    biv['default_sup'] = biv.default + t.ppf([0.975], biv.cont - 1) * biv.ep\n",
    "    biv['default_inf'] = biv.default - t.ppf([0.025], biv.cont - 1) * biv.ep\n",
    "    \n",
    "    biv['logit'] = np.log(biv.default / (1 - biv.default))\n",
    "    biv['logit_sup'] = np.log(biv.default_sup / (1 - biv.default_sup))\n",
    "    biv['logit_inf'] = np.log(biv.default_inf / (1 - biv.default_inf))\n",
    "\n",
    "    general_default_rate = df_filled.default.mean()\n",
    "    general_woe = np.log(df_filled.default.mean() / (1 - df_filled.default.mean()))\n",
    "\n",
    "    biv['woe'] = biv.logit - general_woe\n",
    "    biv['woe_sup'] = biv.logit_sup - general_woe\n",
    "    biv['woe_inf'] = biv.logit_inf - general_woe\n",
    "\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(8, 6))\n",
    "    ax[0].plot(biv[var], biv.woe, ':bo', label='woe')\n",
    "    ax[0].plot(biv[var], biv.woe_sup, 'o:r', label='superior limit')\n",
    "    ax[0].plot(biv[var], biv.woe_inf, 'o:r', label='inferior limit')\n",
    "    \n",
    "    num_cat = biv.shape[0]\n",
    "    ax[0].set_xlim([-.3, num_cat - .7])\n",
    "\n",
    "    ax[0].set_ylabel(\"Weight of Evidence\")\n",
    "    ax[0].legend(bbox_to_anchor=(.83, 1.17), ncol=3)\n",
    "    \n",
    "    ax[0].set_xticks(list(range(num_cat)))\n",
    "    ax[0].set_xticklabels(biv[var], rotation=15)\n",
    "    \n",
    "    ax[1] = biv.cont.plot.bar(ax=ax[1])\n",
    "\n",
    "    # Adding count labels on top of each bar\n",
    "    for p in ax[1].patches:\n",
    "        ax[1].annotate(f'{int(p.get_height())}', \n",
    "                       (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                       ha='center', va='center', \n",
    "                       xytext=(0, 10), \n",
    "                       textcoords='offset points')\n",
    "    \n",
    "    return biv\n",
    "\n",
    "\n",
    "df_struc = df_filled.copy()\n",
    "\n",
    "biv_discrete('income_type', df_struc)\n",
    "\n",
    "df_struc.income_type.replace({'Scholarship Holder': 'Scholarship/P.Servant', 'Public Servant': 'Scholarship/P.Servant'}, inplace=True)\n",
    "biv_discrete('income_type', df_struc)\n",
    "\n",
    "biv_discrete('residents_count', df_struc)\n",
    "\n",
    "df_struc['residents_count'] = df_struc['residents_count'].astype(float)\n",
    "df_struc['residents_count'] = df_struc['residents_count'].apply(lambda x: x if x <= 6 else '>6')\n",
    "df_struc['residents_count'] = df_struc['residents_count'].astype(str)\n",
    "biv_discrete('residents_count', df_struc)\n",
    "\n",
    "def biv_continuous(var, ncat, df):\n",
    "    df['not_default'] = 1 - df.default\n",
    "    cat_srs, bins = pd.qcut(df[var], ncat, retbins=True, precision=0, duplicates='drop')\n",
    "    \n",
    "    # Explicitly set observed=False to silence the warning\n",
    "    g = df.groupby(cat_srs, observed=False)\n",
    "\n",
    "    biv = pd.DataFrame({'qt_not_default': g['not_default'].sum(),\n",
    "                        'qt_default': g['default'].sum(),\n",
    "                        'default': g['default'].mean(), \n",
    "                        var: g[var].mean(), \n",
    "                        'cont': g[var].count()})\n",
    "    \n",
    "    biv['ep'] = (biv.default * (1 - biv.default) / biv.cont) ** .5\n",
    "    biv['default_sup'] = biv.default + t.ppf([0.975], biv.cont - 1) * biv.ep\n",
    "    biv['default_inf'] = biv.default - t.ppf([0.025], biv.cont - 1) * biv.ep\n",
    "    \n",
    "    biv['logit'] = np.log(biv.default / (1 - biv.default))\n",
    "    biv['logit_sup'] = np.log(biv.default_sup / (1 - biv.default_sup))\n",
    "    biv['logit_inf'] = np.log(biv.default_inf / (1 - biv.default_inf))\n",
    "\n",
    "    general_default_rate = df.default.mean()\n",
    "    general_woe = np.log(df.default.mean() / (1 - df.default.mean()))\n",
    "\n",
    "    biv['woe'] = biv.logit - general_woe\n",
    "    biv['woe_sup'] = biv.logit_sup - general_woe\n",
    "    biv['woe_inf'] = biv.logit_inf - general_woe\n",
    "\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(8, 6))\n",
    "    ax[0].plot(biv[var], biv.woe, ':bo', label='woe')\n",
    "    ax[0].plot(biv[var], biv.woe_sup, 'o:r', label='superior limit')\n",
    "    ax[0].plot(biv[var], biv.woe_inf, 'o:r', label='inferior limit')\n",
    "    \n",
    "    num_cat = biv.shape[0]\n",
    "\n",
    "    ax[0].set_ylabel(\"Weight of Evidence\")\n",
    "    ax[0].legend(bbox_to_anchor=(.83, 1.17), ncol=3)\n",
    "    \n",
    "    ax[1] = biv.cont.plot.bar()\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "biv_continuous('employment_time', 15, df_struc)\n",
    "\n",
    "df_struc.loc[df_struc['employment_time']<0,'employment_time'] = -1\n",
    "\n",
    "df_struc = df_struc.drop(columns=['income'])\n",
    "\n",
    "def remove_outliers_zscore(df, z_thresh=3.0):\n",
    "    df_cleaned = df.copy()\n",
    "\n",
    "    removal_counts = {}\n",
    "\n",
    "    for column in df_cleaned.select_dtypes(include=[np.number]).columns:\n",
    "        z_scores = np.abs(stats.zscore(df_cleaned[column]))\n",
    "\n",
    "        outliers = z_scores > z_thresh\n",
    "        num_outliers = outliers.sum()\n",
    "        \n",
    "        if num_outliers > 0:\n",
    "            removal_counts[column] = num_outliers\n",
    "\n",
    "        df_cleaned = df_cleaned[~outliers]\n",
    "\n",
    "    for column, count in removal_counts.items():\n",
    "        print(f\"Removed {count} rows based on outliers in column '{column}'\")\n",
    "\n",
    "    df_cleaned.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return df_cleaned\n",
    "\n",
    "df_outl = remove_outliers_zscore(df_struc, 4.0)\n",
    "\n",
    "---\n",
    "\n",
    "def one_hot_encode(df):\n",
    "    df_encoded = pd.get_dummies(df, drop_first=True)\n",
    "\n",
    "    return df_encoded\n",
    "\n",
    "df_1hot = one_hot_encode(df_outl)\n",
    "\n",
    "df_1hot.columns\n",
    "\n",
    "len(df_1hot.columns)\n",
    "\n",
    "continuous_columns = ['age', 'income', 'employment_time', 'children_count']\n",
    "\n",
    "dummy_columns = [col for col in df_1hot.columns if col not in continuous_columns + ['not_default', 'default']]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_scaled_continuous = pd.DataFrame(scaler.fit_transform(df_1hot[continuous_columns]), columns=continuous_columns)\n",
    "\n",
    "df_combined = pd.concat([df_scaled_continuous, df_1hot[dummy_columns].reset_index(drop=True), df_1hot[['not_default', 'default']].reset_index(drop=True)], axis=1)\n",
    "\n",
    "df_train = df_combined[df_combined['reference_date'] <= '2015-12-31']\n",
    "df_oot = df_combined[df_combined['reference_date'] >= '2016-01-01']\n",
    "\n",
    "df_train = df_train.drop(columns=['not_default', 'reference_date', 'income'], inplace=False)\n",
    "df_oot = df_oot.drop(columns=['not_default', 'reference_date', 'income'], inplace=False)\n",
    "\n",
    "X = df_train.drop(columns=['default'])\n",
    "y = df_train['default']  # This is the response variable\n",
    "\n",
    "pca = PCA(n_components=10)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "df_pca = pd.DataFrame(data=X_pca, columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10'])\n",
    "\n",
    "df_train_final = pd.concat([df_pca, y.reset_index(drop=True)], axis=1)\n",
    "\n",
    "X_ = df_oot.drop(columns=['default'])\n",
    "y_ = df_oot['default']  # This is the response variable\n",
    "pca_ = PCA(n_components=10)\n",
    "X_pca_ = pca_.fit_transform(X_)\n",
    "df_pca_ = pd.DataFrame(data=X_pca_, columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10'])\n",
    "df_oot_final = pd.concat([df_pca_, y_.reset_index(drop=True)], axis=1)\n",
    "\n",
    "cumulative_explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "df_train_final_X = df_train_final.drop(columns=['default'])\n",
    "df_train_final_y = df_train_final['default']\n",
    "\n",
    "df_oot_final_X = df_oot_final.drop(columns=['default'])\n",
    "df_oot_final_y = df_oot_final['default']\n",
    "\n",
    "model = LogisticRegression(class_weight='balanced')\n",
    "model.fit(df_train_final_X, df_train_final_y)\n",
    "\n",
    "predictions_train = model.predict(df_train_final_X)\n",
    "\n",
    "# Convert probabilities to binary outcomes (threshold of 0.5)\n",
    "predicted_classes_train = (predictions_train >= 0.5).astype(int)\n",
    "accuracy_train = accuracy_score(df_train_final_y, predicted_classes_train)\n",
    "print(f'Accuracy for the Train set: {accuracy_train * 100:.2f}%')\n",
    "\n",
    "predictions_oot = model.predict(df_oot_final_X)\n",
    "\n",
    "# Convert probabilities to binary outcomes (threshold of 0.5)\n",
    "predicted_classes_oot = (predictions_oot >= 0.5).astype(int)\n",
    "accuracy_oot = accuracy_score(df_oot_final_y, predicted_classes_oot)\n",
    "print(f'Accuracy for the Out-of-Time set: {accuracy_oot * 100:.2f}%')\n",
    "\n",
    "# Confusion matrix for the training set\n",
    "cm_train = confusion_matrix(df_train_final_y, predictions_train >= 0.5)\n",
    "# Confusion matrix for the OOT set\n",
    "cm_oot = confusion_matrix(df_oot_final_y, predictions_oot >= 0.5)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Training set confusion matrix\n",
    "ConfusionMatrixDisplay(cm_train, display_labels=['No Default', 'Default']).plot(ax=ax[0], cmap='Blues')\n",
    "ax[0].set_title('Training Set Confusion Matrix')\n",
    "\n",
    "# OOT set confusion matrix\n",
    "ConfusionMatrixDisplay(cm_oot, display_labels=['No Default', 'Default']).plot(ax=ax[1], cmap='Greens')\n",
    "ax[1].set_title('OOT Set Confusion Matrix')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "## KS\n",
    "\n",
    "The <b>Kolmogorov-Smirnov (KS)</b> statistic is a metric used to evaluate the discriminatory power of a logistic regression model. It measures the maximum distance between the cumulative distribution functions (CDFs) of the predicted probabilities for the positive and negative classes. A higher KS value indicates better separation between the classes, reflecting the model's effectiveness in distinguishing between them.\n",
    "\n",
    "#### KS Statistic Interpretation\n",
    "\n",
    "The divisions below provide a general guideline for interpreting KS values in the context of model performance:\n",
    "\n",
    "- **KS < 0.1:** Poor separation; the model has limited ability to distinguish between classes.\n",
    "- **0.1 ‚â§ KS < 0.2:** Fair separation; the model shows some ability to distinguish between classes but may need improvement.\n",
    "- **0.2 ‚â§ KS < 0.3:** Good separation; the model effectively discriminates between classes.\n",
    "- **KS ‚â• 0.3:** Excellent separation; the model is highly effective at distinguishing between classes.\n",
    "\n",
    "These divisions are commonly used in industries such as finance and risk modeling to assess the effectiveness of predictive models.\n",
    "\n",
    "\n",
    "prob_train_pos = predictions_train[df_train_final_y == 1]  # Predicted probabilities for positive class\n",
    "prob_train_neg = predictions_train[df_train_final_y == 0]  # Predicted probabilities for negative class\n",
    "ks_stat_train = stats.ks_2samp(prob_train_pos, prob_train_neg).statistic\n",
    "print(f'KS Statistic for Training Set: {ks_stat_train:.4f}')\n",
    "\n",
    "prob_train_pos = predictions_oot[df_oot_final_y == 1]  # Predicted probabilities for positive class\n",
    "prob_train_neg = predictions_oot[df_oot_final_y == 0]  # Predicted probabilities for negative class\n",
    "ks_stat_train = stats.ks_2samp(prob_train_pos, prob_train_neg).statistic\n",
    "print(f'KS Statistic for Out-of-Time Set: {ks_stat_train:.4f}')\n",
    "\n",
    "##### Our KS sits on the \"Good Separation\" category\n",
    "\n",
    "## Gini\n",
    "\n",
    "The **Gini coefficient** is a key metric used to evaluate the discriminatory power of a logistic regression model, showing how effectively the model distinguishes between positive and negative classes. It is closely related to the **ROC Curve** (Receiver Operating Characteristic Curve) and is calculated using the formula:\n",
    "\n",
    "$$\n",
    "\\text{Gini} = 2 \\times \\text{AUC} - 1\n",
    "$$\n",
    "\n",
    "In an ideal scenario, where the model perfectly separates the classes, the ROC curve would touch the top left corner of the plot, yielding an **AUC** of 1 and a **Gini coefficient** of 1, signifying perfect discrimination.\n",
    "\n",
    "\n",
    "### Gini Coefficient Divisions\n",
    "\n",
    "- **Gini < 0.1:** **Very Poor** discrimination; the model has little to no ability to distinguish between classes.\n",
    "- **0.1 ‚â§ Gini < 0.2:** **Poor** discrimination; the model shows some ability to distinguish between classes, but it may not be very effective.\n",
    "- **0.2 ‚â§ Gini < 0.3:** **Fair** discrimination; the model can moderately distinguish between classes but still has room for improvement.\n",
    "- **0.3 ‚â§ Gini < 0.4:** **Good** discrimination; the model is reasonably effective at distinguishing between classes.\n",
    "- **Gini ‚â• 0.4:** **Very Good** to **Excellent** discrimination; the model is highly effective at distinguishing between classes.\n",
    "\n",
    "\n",
    "Let's calculate the Gini and plot the ROC curve.\n",
    "\n",
    "auc_train = roc_auc_score(df_train_final_y, predictions_train)\n",
    "auc_oot = roc_auc_score(df_oot_final_y, predictions_oot)\n",
    "gini_train = 2 * auc_train - 1\n",
    "gini_oot = 2 * auc_oot - 1\n",
    "print(f'Gini Coefficient for Training Set: {gini_train:.4f}')\n",
    "print(f'Gini Coefficient for OOT Set: {gini_oot:.4f}')\n",
    "\n",
    "Our model sits in between <b>Fair</b> and <b>Good</b> discrimination.\n",
    "\n",
    "# ROC Curve for Training Set\n",
    "fpr_train, tpr_train, _ = roc_curve(df_train_final_y, predictions_train)\n",
    "roc_auc_train = auc(fpr_train, tpr_train)\n",
    "\n",
    "# ROC Curve for OOT Set\n",
    "fpr_oot, tpr_oot, _ = roc_curve(df_oot_final_y, predictions_oot)\n",
    "roc_auc_oot = auc(fpr_oot, tpr_oot)\n",
    "\n",
    "# Plotting the ROC Curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Training Set\n",
    "plt.plot(fpr_train, tpr_train, color='blue', lw=2, label=f'Train ROC curve (area = {roc_auc_train:.2f})')\n",
    "\n",
    "# OOT Set\n",
    "plt.plot(fpr_oot, tpr_oot, color='green', lw=2, label=f'OOT ROC curve (area = {roc_auc_oot:.2f})')\n",
    "\n",
    "# Diagonal line\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--', label='Random Guessing')\n",
    "\n",
    "# Axis Labels and Title\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"background-color: #15616d; color: white; padding: 20px; border-radius: 10px;\">\n",
    "    <h1 style=\"text-align: center; font-family: 'Arial', sans-serif;\">üõ†Ô∏è Pipeline</h1>\n",
    "    <p style=\"font-size: 1.2em; text-align: center;\">\n",
    "        Here, we'll refactor part of our code into a scikit-learn Pipeline for improved reusability and streamlined processing.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "## Criar um pipeline utilizando o sklearn pipeline \n",
    "\n",
    "## Pr√© processamento\n",
    "\n",
    "### Substitui√ß√£o de nulos (nans)\n",
    "\n",
    "Existe nulos na base? √© dado num√©rico ou categ√≥rico? qual o valor de substitui√ß√£o? m√©dia? valor mais frequente? etc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Remo√ß√£o de outliers\n",
    "\n",
    "Como identificar outlier? Substituir o outlier por algum valor? Remover a linha?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Sele√ß√£o de vari√°veis\n",
    "\n",
    "Qual tipo de t√©cnica? Boruta? Feature importance? \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Redu√ß√£o de dimensionalidade (PCA)\n",
    "\n",
    "Aplicar PCA para reduzir a dimensionalidade para 5\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cria√ß√£o de dummies\n",
    "\n",
    "Aplicar o get_dummies() ou onehotencoder() para transformar colunas cat√©goricas do dataframe em colunas de 0 e 1. \n",
    "- sexo\n",
    "- posse_de_veiculo\n",
    "- posse_de_imovel\n",
    "- tipo_renda\n",
    "- educacao\n",
    "- estado_civil\n",
    "- tipo_residencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline \n",
    "\n",
    "Crie um pipeline contendo essas fun√ß√µes.\n",
    "\n",
    "preprocessamento()\n",
    "- substituicao de nulos\n",
    "- remo√ß√£o outliers\n",
    "- PCA\n",
    "- Cria√ß√£o de dummy de pelo menos 1 vari√°vel (posse_de_veiculo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinar um modelo de regress√£o logistica com o resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvar o pickle file do modelo treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "nome_arquivo = 'model_final.pkl'\n",
    "pickle.dump(model, open(nome_arquivo, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pycaret na base de dados \n",
    "\n",
    "Utilize o pycaret para pre processar os dados e rodar o modelo **lightgbm**. Fa√ßa todos os passos a passos da aula e gere os gr√°ficos finais. E o pipeline de toda a transforma√ß√£o.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'credit_scoring.ftr'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[670], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_feather\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcredit_scoring.ftr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\feather_format.py:125\u001b[0m, in \u001b[0;36mread_feather\u001b[1;34m(path, columns, use_threads, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension_types\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401,E501\u001b[39;00m\n\u001b[0;32m    123\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[1;32m--> 125\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m    127\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_backend \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_pyarrow_string_dtype():\n\u001b[0;32m    129\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m feather\u001b[38;5;241m.\u001b[39mread_feather(\n\u001b[0;32m    130\u001b[0m             handles\u001b[38;5;241m.\u001b[39mhandle, columns\u001b[38;5;241m=\u001b[39mcolumns, use_threads\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m(use_threads)\n\u001b[0;32m    131\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:872\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    864\u001b[0m             handle,\n\u001b[0;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    869\u001b[0m         )\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    873\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    875\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'credit_scoring.ftr'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_feather('credit_scoring.ftr')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.classification import *\n",
    "models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx = create_model('xxx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvar o arquivo do modelo treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto Final\n",
    "\n",
    "1. Subir no GITHUB todos os jupyter notebooks/c√≥digos que voc√™ desenvolveu nesse ultimo m√≥dulo\n",
    "1. Gerar um arquivo python (.py) com todas as fun√ß√µes necess√°rias para rodar no streamlit a escoragem do arquivo de treino\n",
    "    - Criar um .py\n",
    "    - Criar um carregador de csv no streamlit \n",
    "    - Subir um csv no streamlit \n",
    "    - Criar um pipeline de pr√© processamento dos dados\n",
    "    - Utilizar o modelo treinado para escorar a base \n",
    "        - nome_arquivo = 'model_final.pkl'\n",
    "1. Gravar um v√≠deo da tela do streamlit em funcionamento (usando o pr√≥prio streamlit (temos aula disso) ou qlqr outra forma de grava√ß√£o).\n",
    "1. Subir no Github o v√≠deo de funcionamento da ferramenta como README.md.\n",
    "1. Subir no Github os c√≥digos desenvolvidos. \n",
    "1. Enviar links do github para o tutor corrigir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "√çndice",
   "title_sidebar": "Conte√∫do",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
